<!DOCTYPE html><html lang=""><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>用一个流程解决所有机器学习难题的学习 | CS Learning of CHNwentao</title><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css?version=1.3"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?js?c4c99183a6a02c53d4760ae1995c3b38";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL = { 
  root: '/',
  algolia: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  localSearch: {"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}"},"path":"search.xml"}
} </script><link rel="icon" href="/favicon.ico"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.</span> <span class="toc-text">参考</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">2.</span> <span class="toc-text">预备知识</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.1.</span> <span class="toc-text">保持训练数据和检验数据中标签的一致性 ：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.2.</span> <span class="toc-text">one-hot 编码 :</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">3.</span> <span class="toc-text">数据</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">4.</span> <span class="toc-text">标签的类型</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">5.</span> <span class="toc-text">评估价值</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">6.</span> <span class="toc-text">资料库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">7.</span> <span class="toc-text">机器学习框架</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">7.1.</span> <span class="toc-text">定义问题。</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">7.2.</span> <span class="toc-text">数据集划分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">7.3.</span> <span class="toc-text">不同变量的处理</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-number">7.3.1.</span> <span class="toc-text">数据变量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-number">7.3.2.</span> <span class="toc-text">种类变量</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#undefined"><span class="toc-number">7.3.3.</span> <span class="toc-text">文本变量</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">7.4.</span> <span class="toc-text">stacker</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">7.5.</span> <span class="toc-text">特征的选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">7.6.</span> <span class="toc-text">模型的选择</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">7.7.</span> <span class="toc-text">超参数优化</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="http://7xiuu0.com1.z0.glb.clouddn.com/17-11-23/22222257.jpg"></div><div class="author-info__name text-center">Wente Guo</div><div class="author-info__description text-center">一个学习的博客网站，专注于计算机科学、机器学习、数据挖掘等的 技术分享、相关咨询及经验总结</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">131</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">181</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">19</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container" style="background-image: url(true);"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">CS Learning of CHNwentao</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span></div><div id="post-info"><div id="post-title">用一个流程解决所有机器学习难题的学习</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2016-07-31</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"> </i><a class="post-meta__categories" href="/categories/机器学习/">机器学习</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2,107</span><span class="post-meta__separator">|</span><span>Reading time: 7 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div id="post-content"><h2>参考</h2>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p><a href="http://blog.kaggle.com/2016/07/21/approaching-almost-any-machine-learning-problem-abhishek-thakur/" target="_blank" rel="noopener">Approaching (Almost) Any Machine Learning Problem | Abhishek Thakur</a>
&lt;!-- more --&gt;</p>
<h2>预备知识</h2>
<h3>保持训练数据和检验数据中标签的一致性 ：</h3>
<p>就是在模型训练的时候，训练的数据和测试的数据label的分布是一样的。也就是是分层抽样的意思， 好处是：在训练中不会有倾向。</p>
<h3>one-hot 编码 :</h3>
<p>又称一位有效编码，其方法是使用N位状态寄存器来对N个状态进行编码，每个状态都由他独立的寄存器位，并且在任意时候，其中只有一位有效。
可以这样理解，对于每一个特征，如果它有m个可能值，那么经过独热编码后，就变成了m个二元特征。并且，这些特征互斥，每次只有一个激活。因此，数据会变成稀疏的。</p>
<p>这样做的好处主要有：</p>
<ul>
<li>
<p>解决了分类器不好处理属性数据的问题</p>
</li>
<li>
<p>在一定程度上也起到了扩充特征的作用
我们基于python和Scikit-learn写一个简单的例子：
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> preprocessing</span><br><span class="line">enc = preprocessing.OneHotEncoder()</span><br><span class="line">enc.fit([[<span class="number">0</span>, <span class="number">0</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">1</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">2</span>, <span class="number">1</span>], [<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>]])</span><br><span class="line">enc.transform([[<span class="number">0</span>, <span class="number">1</span>, <span class="number">3</span>]]).toarray()</span><br><span class="line"></span><br><span class="line">---out---</span><br><span class="line"></span><br><span class="line">array([[ <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">0.</span>,  <span class="number">1.</span>]])</span><br></pre></td></tr></table></figure></p>
</li>
</ul>
<h2>数据</h2>
<p>在采用机器学习模型前， 数据必须要转化成一个列表（Tabular）的形式。这是最消耗时间，也是最困难的，其过程如下：
<img src="01.png" alt=""></p>
<h2>标签的类型</h2>
<p>这些标签定义了所要解决的问题，可以有不同的形式：</p>
<ul>
<li>单行，二进制值（分类问题，一个样本只属于一个种类，且种类总数只有2个）</li>
<li>单行，真值（回归问题，预测唯一值）</li>
<li>多行，二进制值（分类问题，一个样本属于一个分类，但是有2个或者多个种类）</li>
<li>多行，真值（回归问题，预测多值）</li>
<li>多个标签（分类问题，一个样本可以属于不同的种类）</li>
</ul>
<h2>评估价值</h2>
<p>对于任何机器学习难题，我们必须知道要怎样评估自己的研究结果，或者说，评估的价值和对象是什么。为了防止二进制分类中的 <strong>负偏（skewed）的问题</strong> ，我们通常会选择在运行特征曲线（ROC AUC 或者简单的 AUC）的接收器（receiver）下方区域进行评估。</p>
<p>在多标签和多类型分类难题中，我们通常选择分类交互熵，或者多类型的 log loss ，以及在回归问题中降低平方误差。</p>
<h2>资料库</h2>
<p>观看和进行数据处理：Pandas
各种机器学习模型：Scikit-learn
最好的梯度渐进数据库看：xgboots
神经网络：keras
绘图数据：matplotlib
监控进度：tqdm</p>
<h2>机器学习框架</h2>
<p><img src="02.png" alt=""></p>
<h3>定义问题。</h3>
<p>这可以通过标签确定。研究者首先要明确，你的问题是二进制的、多种类的、多标签分类的还是回归问题。在定义了问题之后，我们可以把数据分为两个不同的部分，如下文描述，一部分是训练数据，另一部分是检验数据。</p>
<h3>数据集划分</h3>
<p>把数据进行“训练”和“检验”的区分，必须要根据数据标签进行。在所有的分类问题中，都要试用分层分割。在Python中，你可以使用Scikit-learn来轻易地做到。</p>
<p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">form sklearn.cross_validation <span class="keyword">import</span> StratifiedKFold</span><br><span class="line">eval_size = <span class="number">0.10</span></span><br><span class="line">kf = StratifiedKFold(y, round(<span class="number">1.</span> / eval_size))</span><br><span class="line">train_indices, valid_indices = next(iter(kf))</span><br><span class="line">X_train, y_train = X[train_indices], y[train_indices]</span><br><span class="line">X_valid, y_valid = X[valid_indices], y[valid_indices]</span><br></pre></td></tr></table></figure></p>
<p>在回归任务中，一个简单的K-Fold分割应该就足够了。但是，有一些复杂的方法，倾向于 <strong>保持训练数据和检验数据中标签的一致性</strong>。
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">form sklearn.cross_validation <span class="keyword">import</span> KFold</span><br><span class="line">eval_size = <span class="number">0.10</span></span><br><span class="line">kf = KFold(y, round(<span class="number">1.</span> / eval_size))</span><br><span class="line">train_indices, valid_indices = next(iter(kf))</span><br><span class="line">X_train, y_train = X[train_indices], y[train_indices]</span><br><span class="line">X_valid, y_valid = X[valid_indices], y[valid_indices]</span><br></pre></td></tr></table></figure></p>
<p>在上面的例子中，我选择 eval_size 或者 size of the validation set作为全部数据的10%，但是，你可以根据自己拥有的数据选择赋值。</p>
<p>在数据分层完成后，先把它们搁在一旁不要碰。在训练数据集上的任何操作都要保存，最后会运用到检验数据集中。而检验数据集，在任何情况下都不应该跟训练数据集混淆。如果能做到这样，就会得到非常好的分数。否则，你可能建立的是一个没多大用，且过度拟合的模型。</p>
<h3>不同变量的处理</h3>
<h4>数据变量</h4>
<p>首先把数据变量排除。这些变量并不需要任何的处理，我们可以使用标准的机器学习模型来处理。</p>
<h4>种类变量</h4>
<p>在种类变量的处理上，我们有两个方式：</p>
<p>把种类数据变成标签:
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> LabelEncoder</span><br><span class="line"></span><br><span class="line">lbl_enc = LabelEncoder()</span><br><span class="line">lbl_enc.fit(catagorical_features)</span><br><span class="line">xtrain_cat = lbl_enc.trainsform(x.train[catagorical_features])</span><br></pre></td></tr></table></figure></p>
<p>把标签转化成二进制变量（one-hot 编码
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> OneHotEncoder</span><br><span class="line"></span><br><span class="line">lbl_ohe = OneHotEncoder()</span><br><span class="line">lbl_ohe.fit(catagorical_features)</span><br><span class="line">xtrain_cat = llbl_ohe.trainsform(x.train[catagorical_features])</span><br></pre></td></tr></table></figure></p>
<h4>文本变量</h4>
<p>可以使用<code>CountVectorizer</code>或<code>TfidfVectorizer</code> ：</p>
<p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">ctv = CountVectorizer()</span><br><span class="line">text_data_train = ctv.fit_transtrom(text_data_train)</span><br></pre></td></tr></table></figure></p>
<p>TfidfVectorizer的表现一直都比其他工具要好，据我观察，以下的参数几乎每次都有效：</p>
<p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfVectorizer</span><br><span class="line">tfv = TfidfVectorizer(min_df=<span class="number">3</span>, max_features=<span class="keyword">None</span>, strip_accents=<span class="string">'unicode'</span>, analyzer=<span class="string">'word'</span>, token_pattern=<span class="string">r'\w&#123;1,&#125;'</span>,ngram_range=(<span class="number">1</span>, <span class="number">2</span>), use_id=<span class="number">1</span>, sublinear_tf=<span class="number">1</span>, stop_words=<span class="string">'english'</span>)</span><br></pre></td></tr></table></figure></p>
<h3>stacker</h3>
<p>叠式储存器（stacker） 模块。Stacker并不是一个模型stacker，而是一个特征stacker。在上文提到的处理步骤完成后，不同的特征可以被结合起来，用到Stacker模块中。</p>
<p>你可以使用numpyhstack或者sparse hstack把所有的特征水平堆叠起来，这取决你拥有的是稀疏或者紧密特征。</p>
<p>注意，此时数据没有被标准化，我们在上面的特征中不能使用线性模型。要使用线性模型，你可以从scikit-learn上使用Normalizer或StandardScaler。这些规范化的方法只有在紧密特征中才起作用，在稀疏特征中不会有好的效果。</p>
<h3>特征的选择</h3>
<p>特征的选择，有多种方式可以实现。最常见的是贪婪特征的选择（正向或反向）。在贪婪特征的选择上，我们选择一个特征，训练一个模型并用一个修正的评估值来评估模型的性能。我们不断增加或者移除一个又一个特征，并逐步记录模型的表现。随后，我们选出得分最高的特征。必须说明的是，这种方法并不是完美的，需要根据要求改变或修正。</p>
<p>其他更快的特征选择方法包括：从一个模型中选择最佳的特征。我们可以观察一个逻辑模型的稀疏，或者训练一个随机森林，来选择最佳的特征，然后在其他的机器学习模型上使用。</p>
<p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">form sklearn.ensemble <span class="keyword">import</span> RandomForestClassifier</span><br><span class="line"></span><br><span class="line">clf = RandomForestClassifer(n_estimators=<span class="number">100</span>, n_jobs=<span class="number">-1</span>)</span><br><span class="line">clf.fit(X, y)</span><br><span class="line">X_selected = clf.transform(X)</span><br></pre></td></tr></table></figure></p>
<p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xgboost <span class="keyword">as</span> xgb</span><br><span class="line"></span><br><span class="line">params=&#123;&#125;</span><br><span class="line">model = xgb.train(params, dtrain, num_bost_rund=<span class="number">100</span>)</span><br><span class="line">sorted(model.get_fscore().items(), key=<span class="keyword">lambda</span> t: =-t[<span class="number">1</span>])</span><br></pre></td></tr></table></figure></p>
<p>记得保持较少数量的Estimator，并对超参数进行最小优化，这样你就不会过度拟合。</p>
<p>特征的选择也可以通过Gradient Boosting Machines来实现。如果我们使用xgboost而不是在 scikit-learn中使用GBM时，效果会很好。因为xgboost速度更快、可扩展性更高。</p>
<p>我们也可以使用RandomForestClassifier 、RandomForestRegressor 和xgboost，在稀疏数据集中进行特征选择。</p>
<p>另一个较为流行的方法是基于chi-2的特征选择。
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 在这儿，我们使用Chi2和 SelectKBes从数据中选择20个特征。这也变成了我们希望优化，来提升机器学习模型结果的超参数</span></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"></span><br><span class="line">skb = SelectKBest(chi2, k=<span class="number">20</span>)</span><br><span class="line">skb.fit_transfrom(X,y)</span><br></pre></td></tr></table></figure></p>
<h3>模型的选择</h3>
<p>![](scikit-learn algorithm cheat sheet copy.png)</p>
<h3>超参数优化</h3>
<p>我该优化哪些参数？我怎样才能选到最匹配的参数？这是人们考虑得最多的两个问题。没有在大量的数据库上体验过不同的模型和参数，是无法回答这一问题的。还有一点，许多人并不愿意分享这方面的经验。所幸我还有一点经验，也愿意分享：</p>
<p><img src="03.png" alt="RS*指不能确定合适的值"></p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info">Wente Guo</span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info">http://chnwentao.com/2016/07/机器学习/ML-总结-用一个框架解决所有机器学习难题的学习/</span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/机器学习/">机器学习</a><a class="post-meta__tags" href="/tags/scikit-learn/">scikit-learn</a><a class="post-meta__tags" href="/tags/kaggle/">kaggle</a></div></article><nav id="pagination"><div class="prev-post pull-left"><a href="/2016/09/机器学习/ML-总结-TF-IDF算法/"><i class="fa fa-chevron-left">  </i><span>TF-IDF介绍及python实例</span></a></div><div class="next-post pull-right"><a href="/2016/07/机器学习/ML-总结-GBRT/"><span>基于决策树的组合算法之GBRT</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">©2015 - 2017 By Wente Guo</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.3"></script><script src="/js/fancybox.js?version=1.3"></script><script src="/js/sidebar.js?version=1.3"></script><script src="/js/copy.js?version=1.3"></script><script src="/js/fireworks.js?version=1.3"></script><script src="/js/transition.js?version=1.3"></script><script src="/js/scroll.js?version=1.3"></script><script src="/js/head.js?version=1.3"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>