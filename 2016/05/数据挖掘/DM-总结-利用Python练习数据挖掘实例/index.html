<!DOCTYPE html><html lang=""><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>利用Python练习数据挖掘实例 | CS Learning of CHNwentao</title><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css?version=1.3"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?js?c4c99183a6a02c53d4760ae1995c3b38";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL = { 
  root: '/',
  algolia: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  localSearch: {"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}"},"path":"search.xml"}
} </script><link rel="icon" href="/favicon.ico"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.</span> <span class="toc-text">数据导入和可视化</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">1.1.</span> <span class="toc-text">数据导入</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">1.2.</span> <span class="toc-text">可视化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">2.</span> <span class="toc-text">分类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.1.</span> <span class="toc-text">拆分训练集和测试集</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.2.</span> <span class="toc-text">评价：精确度</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.3.</span> <span class="toc-text">评价：混淆矩阵</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.4.</span> <span class="toc-text">评价：分类器性能报告</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.5.</span> <span class="toc-text">评价：交叉验证</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">3.</span> <span class="toc-text">聚类</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">3.1.</span> <span class="toc-text">评估：完整性得分、同质性得分</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">3.2.</span> <span class="toc-text">评估：结果可视化</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">4.</span> <span class="toc-text">回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">4.1.</span> <span class="toc-text">评估：结果可视化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">4.2.</span> <span class="toc-text">评估：均方误差</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">5.</span> <span class="toc-text">降维</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">5.1.</span> <span class="toc-text">转换计算如下：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">5.2.</span> <span class="toc-text">结果可视化</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">5.3.</span> <span class="toc-text">信息量判断</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="http://7xiuu0.com1.z0.glb.clouddn.com/17-11-23/22222257.jpg"></div><div class="author-info__name text-center">Wente Guo</div><div class="author-info__description text-center">一个学习的博客网站，专注于计算机科学、机器学习、数据挖掘等的 技术分享、相关咨询及经验总结</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">134</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">184</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">19</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container" style="background-image: url(true);"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">CS Learning of CHNwentao</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span></div><div id="post-info"><div id="post-title">利用Python练习数据挖掘实例</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2016-05-19</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"> </i><a class="post-meta__categories" href="/categories/数据挖掘/">数据挖掘</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">2,898</span><span class="post-meta__separator">|</span><span>阅读时长: 12 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div id="post-content"><p><!-- hexo-inject:begin --><!-- hexo-inject:end -->参考：https://dzone.com/refcardz/data-mining-discovering-and
主要内容有（嘿嘿，手动严肃）：
导入和可视化数据；数据分类；使用回归分析和相关测量法发现数据之间的关系；数据降维以压缩和可视化数据带来的信息</p>
<p>文章中的代码基于四个主要的Python数据分析和处理的类库：numpy，matplotlib，sklearn和networkx。
&lt;!-- more --&gt;</p>
<h2>数据导入和可视化</h2>
<h3>数据导入</h3>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从网上下载数据并保存</span></span><br><span class="line"><span class="keyword">import</span> urllib2</span><br><span class="line">url = <span class="string">'http://aima.cs.berkeley.edu/data/iris.csv'</span></span><br><span class="line">u = urllib2.urlopen(url)</span><br><span class="line">localFile = open(<span class="string">'iris.csv'</span>,<span class="string">'w'</span>)</span><br><span class="line">localFile.write(u.read())</span><br><span class="line">localFile.close()</span><br></pre></td></tr></table></figure></p>
<p>数据集以CSV(逗号分割值）的格式存储。CSV文件可以很方便的转化并把其中的信息存储为适合的数据结构。此数据集有5列，前4列包含着特征值，最后一列代表着样本类型。CSV文件很容易被numpy类库的genfromtxt方法解析：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建了一个包含特征值的矩阵以及一个包含样本类型的向量。</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> genfromtxt, zeros</span><br><span class="line"><span class="comment"># read the first 4 columns</span></span><br><span class="line">data = genfromtxt(<span class="string">'iris.csv'</span>,delimiter=<span class="string">','</span>,usecols=(<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>))</span><br><span class="line"><span class="comment"># read the fifth column</span></span><br><span class="line">target = genfromtxt(<span class="string">'iris.csv'</span>,delimiter=<span class="string">','</span>,usecols=(<span class="number">4</span>),dtype=str)</span><br></pre></td></tr></table></figure></p>
<p>在上面的例子中我们创建了一个包含特征值的矩阵以及一个包含样本类型的向量。我们可以通过查看我们加载的数据结构的shape值来确认数据集的大小：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过查看我们加载的数据结构的shape值来确认数据集的大小：</span></span><br><span class="line"><span class="keyword">print</span> data.dtype</span><br><span class="line"><span class="keyword">print</span> data.shape</span><br><span class="line">(<span class="number">150</span>, <span class="number">4</span>)</span><br><span class="line"><span class="keyword">print</span> target.shape</span><br><span class="line">(<span class="number">150</span>,)</span><br><span class="line"><span class="comment"># 查看我们有多少种样本类型以及它们的名字：</span></span><br><span class="line"><span class="keyword">print</span> set(target) <span class="comment"># build a collection of unique elements</span></span><br></pre></td></tr></table></figure></p>
<pre><code>float64
(150, 4)
(150,)
set(['setosa', 'versicolor', 'virginica'])
</code></pre>
<h3>可视化</h3>
<p>当我们处理新数据的时候，一项很重要的任务是尝试去理解数据包含的信息以及它的组织结构。可视化可以灵活生动的展示数据，帮助我们深入理解数据。</p>
<p>使用pylab类库（matplotlib的接口）的plotting方法可以建一个二维散点图让我们在两个维度上分析数据集的两个特征值：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 建立一个二维散点图让我们在两个维度上分析数据集的两个特征值：</span></span><br><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> plot, show</span><br><span class="line">% matplotlib inline</span><br><span class="line">plot(data[target==<span class="string">'setosa'</span>,<span class="number">0</span>],data[target==<span class="string">'setosa'</span>,<span class="number">2</span>],<span class="string">'bo'</span>)</span><br><span class="line">plot(data[target==<span class="string">'versicolor'</span>,<span class="number">0</span>],data[target==<span class="string">'versicolor'</span>,<span class="number">2</span>],<span class="string">'ro'</span>)</span><br><span class="line">plot(data[target==<span class="string">'virginica'</span>,<span class="number">0</span>],data[target==<span class="string">'virginica'</span>,<span class="number">2</span>],<span class="string">'go'</span>)</span><br><span class="line">show()</span><br></pre></td></tr></table></figure></p>
<p><img src="/output_8_0.png" alt="png"></p>
<p>在上图中有150个点，不同的颜色代表不同的类型；蓝色点代表山鸢尾，红色点代表变色鸢尾，绿色点代表维吉尼亚鸢尾。</p>
<p>另一种常用的查看数据的方法是分特性绘制直方图。在本例中，既然数据被分为三类，我们就可以比较每一类的分布特征。下面这个代码可以绘制数据中每一类型的第一个特性（花萼的长度）：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> pylab <span class="keyword">import</span> figure, subplot, hist, xlim, show</span><br><span class="line">xmin = min(data[:,<span class="number">0</span>])</span><br><span class="line">xmax = max(data[:,<span class="number">0</span>])</span><br><span class="line">figure()</span><br><span class="line">subplot(<span class="number">411</span>) <span class="comment"># distribution of the setosa class (1st, on the top)</span></span><br><span class="line">hist(data[target==<span class="string">'setosa'</span>,<span class="number">0</span>],color=<span class="string">'b'</span>,alpha=<span class="number">.7</span>)</span><br><span class="line">xlim(xmin,xmax)</span><br><span class="line">subplot(<span class="number">412</span>) <span class="comment"># distribution of the versicolor class (2nd)</span></span><br><span class="line">hist(data[target==<span class="string">'versicolor'</span>,<span class="number">0</span>],color=<span class="string">'r'</span>,alpha=<span class="number">.7</span>)</span><br><span class="line">xlim(xmin,xmax)</span><br><span class="line">subplot(<span class="number">413</span>) <span class="comment"># distribution of the virginica class (3rd)</span></span><br><span class="line">hist(data[target==<span class="string">'virginica'</span>,<span class="number">0</span>],color=<span class="string">'g'</span>,alpha=<span class="number">.7</span>)</span><br><span class="line">xlim(xmin,xmax)</span><br><span class="line">subplot(<span class="number">414</span>) <span class="comment"># global histogram (4th, on the bottom)</span></span><br><span class="line">hist(data[:,<span class="number">0</span>],color=<span class="string">'y'</span>,alpha=<span class="number">.7</span>)</span><br><span class="line">xlim(xmin,xmax)</span><br><span class="line">show()</span><br></pre></td></tr></table></figure></p>
<p><img src="/output_11_0.png" alt="png"></p>
<h2>分类</h2>
<p>sklearn类库包含很多分类器的实现，我们将会用高斯朴素贝叶斯来分析我们在第一章载入的鸢尾花数据，包含山鸢尾、变色鸢尾和维吉尼亚鸢尾。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 把字符串数组转型成整型数据：</span></span><br><span class="line">t = zeros(len(target))</span><br><span class="line">t[target == <span class="string">'setosa'</span>] = <span class="number">1</span></span><br><span class="line">t[target == <span class="string">'versicolor'</span>] = <span class="number">2</span></span><br><span class="line">t[target == <span class="string">'virginica'</span>] = <span class="number">3</span></span><br></pre></td></tr></table></figure></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 分类器实例化和训练分类器的准备了：</span></span><br><span class="line"><span class="keyword">from</span> sklearn.naive_bayes <span class="keyword">import</span> GaussianNB</span><br><span class="line">classifier = GaussianNB()</span><br><span class="line">classifier.fit(data,t) <span class="comment"># training on the iris dataset</span></span><br></pre></td></tr></table></figure></p>
<pre><code>GaussianNB()
</code></pre>
<p>很简单的检测：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 很简单的检测：</span></span><br><span class="line"><span class="comment"># 分类器可以由predict方法完成，并且只要输出一个样例就可以很简单的检测：</span></span><br><span class="line"><span class="keyword">print</span> classifier.predict(data[<span class="number">0</span>])</span><br><span class="line"><span class="keyword">print</span> t[<span class="number">0</span>]</span><br></pre></td></tr></table></figure></p>
<pre><code>[ 1.]
1.0


/Users/wente/anaconda/lib/python2.7/site-packages/sklearn/utils/validation.py:386: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and willraise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.
  DeprecationWarning)
</code></pre>
<h3>拆分训练集和测试集</h3>
<p>上面的测试使用了训练集的数据，但是在广泛的样本上评估分类器并且使用非训练环节的数据测试是很重要的。最终我们通过从源数据集中随机抽取样本把数据分为训练集和测试集。我们将会使用训练集的数据来训练分类器，并使用测试集的数据来测试分类器。train_test_split方法正是实现此功能的：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 从样本中随机的按比例选取train data和test data。</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> cross_validation</span><br><span class="line">train, test, t_train, t_test = cross_validation.train_test_split(data, t, test_size=<span class="number">0.4</span>, random_state=<span class="number">0</span>)</span><br></pre></td></tr></table></figure></p>
<p>数据集被分一分为二，测试集被指定为源数据的40%（命名为test_size），random_state是随机数的种子。不同的种子会造成不同的随机采样结果。相同的种子采样结果相同。</p>
<p>下面就是利用其反复训练我们的分类器并输出精确度</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">classifier.fit(train,t_train) <span class="comment"># train</span></span><br></pre></td></tr></table></figure></p>
<pre><code>GaussianNB()
</code></pre>
<h3>评价：精确度</h3>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">print</span> classifier.score(test,t_test) <span class="comment"># test</span></span><br></pre></td></tr></table></figure></p>
<pre><code>0.933333333333
</code></pre>
<p>上面得出精确度为93%。一个分类器的精确度是通过正确分类样本的数量除以总样本的数量得出的。也就是说，它意味着我们正确预测的比例。</p>
<h3>评价：混淆矩阵</h3>
<p>另一个估计分类器表现的工具叫做混淆矩阵。在此矩阵中每列代表一个预测类的实例，每行代表一个实际类的实例。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="keyword">print</span> confusion_matrix(classifier.predict(test),t_test)</span><br></pre></td></tr></table></figure></p>
<pre><code>[[16  0  0]
 [ 0 23  4]
 [ 0  0 17]]
</code></pre>
<p>混淆矩阵的评价：所有正确的预测都在表格的对角线上，的错误就即对角线以外的非零值。
在这个混淆矩阵中我们可以看到所有山鸢尾和维吉尼亚鸢尾都被正确的分类了，但是实际上应该是26个的变色鸢尾，系统却预测其中4个是维吉尼亚鸢尾。如果</p>
<h3>评价：分类器性能报告</h3>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> classification_report</span><br><span class="line"><span class="keyword">print</span> classification_report(classifier.predict(test), t_test, target_names=[<span class="string">'setosa'</span>, <span class="string">'versicolor'</span>, <span class="string">'virginica'</span>])</span><br></pre></td></tr></table></figure></p>
<pre><code>             precision    recall  f1-score   support

     setosa       1.00      1.00      1.00        16
 versicolor       1.00      0.85      0.92        27
  virginica       0.81      1.00      0.89        17

avg / total       0.95      0.93      0.93        60
</code></pre>
<p>以下是该报告使用到的方法总结：
Precision：正确预测的比例
Recall（或者叫真阳性率）：正确识别的比例
F1-Score：precision和recall的调和平均数</p>
<h3>评价：交叉验证</h3>
<p>思想：多次将数据分为不同的训练集和测试集，最终分类器评估选取多次预测的平均值。方法：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cross_validation <span class="keyword">import</span> cross_val_score</span><br><span class="line"><span class="comment"># cross validation with 6 iterations</span></span><br><span class="line">scores = cross_val_score(classifier, data, t, cv=<span class="number">6</span>)</span><br><span class="line"><span class="keyword">print</span> scores</span><br></pre></td></tr></table></figure></p>
<pre><code>[ 0.92592593  1.          0.91666667  0.91666667  0.95833333  1.        ]
</code></pre>
<p>输出是每次模型迭代产生的精确度的数组。我们可以很容易计算出平均精确度：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> mean</span><br><span class="line"><span class="keyword">print</span> mean(scores)</span><br></pre></td></tr></table></figure></p>
<pre><code>0.952932098765
</code></pre>
<h2>聚类</h2>
<p>是一种无监督数据分析，下面就是最著名的聚类工具k-means算法</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.cluster <span class="keyword">import</span> KMeans</span><br><span class="line">kmeans = KMeans(n_clusters=<span class="number">3</span>, init=<span class="string">'random'</span>) <span class="comment"># initialization</span></span><br><span class="line">kmeans.fit(data) <span class="comment"># actual execution</span></span><br></pre></td></tr></table></figure></p>
<pre><code>KMeans(copy_x=True, init='random', max_iter=300, n_clusters=3, n_init=10,
    n_jobs=1, precompute_distances='auto', random_state=None, tol=0.0001,
    verbose=0)
</code></pre>
<p>上面运行k-measn算法并把数据分为三个群集（参数n_clusters所指定的）。下面使用模型把每一个样本分配到三个群集中：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">c = kmeans.predict(data)</span><br></pre></td></tr></table></figure></p>
<h3>评估：完整性得分、同质性得分</h3>
<p>估计群集的结果，与使用完整性得分和同质性得分计算而得的标签作比较：</p>
<p>大部分数据点属于一个给定的类并且属于同一个群集，那么完整性得分就趋向于1。
当所有群集都几乎只包含某个单一类的数据点时同质性得分就趋向于1.</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> completeness_score, homogeneity_score</span><br><span class="line"><span class="keyword">print</span> completeness_score(t,c)</span><br><span class="line"><span class="keyword">print</span> homogeneity_score(t,c)</span><br></pre></td></tr></table></figure></p>
<pre><code>0.764986151449
0.751485402199
</code></pre>
<h3>评估：结果可视化</h3>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">figure()</span><br><span class="line">subplot(<span class="number">211</span>) <span class="comment"># top figure with the real classes</span></span><br><span class="line">plot(data[t==<span class="number">1</span>,<span class="number">0</span>],data[t==<span class="number">1</span>,<span class="number">2</span>],<span class="string">'bo'</span>)</span><br><span class="line">plot(data[t==<span class="number">2</span>,<span class="number">0</span>],data[t==<span class="number">2</span>,<span class="number">2</span>],<span class="string">'ro'</span>)</span><br><span class="line">plot(data[t==<span class="number">3</span>,<span class="number">0</span>],data[t==<span class="number">3</span>,<span class="number">2</span>],<span class="string">'go'</span>)</span><br><span class="line">subplot(<span class="number">212</span>) <span class="comment"># bottom figure with classes assigned automatically</span></span><br><span class="line">plot(data[c==<span class="number">2</span>,<span class="number">0</span>],data[c==<span class="number">2</span>,<span class="number">2</span>],<span class="string">'bo'</span>,alpha=<span class="number">.7</span>)</span><br><span class="line">plot(data[c==<span class="number">1</span>,<span class="number">0</span>],data[c==<span class="number">1</span>,<span class="number">2</span>],<span class="string">'go'</span>,alpha=<span class="number">.7</span>)</span><br><span class="line">plot(data[c==<span class="number">0</span>,<span class="number">0</span>],data[c==<span class="number">0</span>,<span class="number">2</span>],<span class="string">'mo'</span>,alpha=<span class="number">.7</span>)</span><br><span class="line">show()</span><br></pre></td></tr></table></figure></p>
<p><img src="/output_41_0.png" alt="png"></p>
<p>观察此图我们可以看到，底部左侧的群集可以被k-means完全识别，然而顶部的两个群集有部分识别错误。</p>
<h2>回归</h2>
<p>回归是一个用于预测变量之间函数关系调查的方法。例如，我们有两个变量，一个被认为是解释，一个被认为是依赖。我们希望使用模型描述两者的关系。当这种关系是一条线的时候就称为线性回归。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 构建数据集</span></span><br><span class="line"><span class="keyword">from</span> numpy.random <span class="keyword">import</span> rand</span><br><span class="line">x = rand(<span class="number">40</span>,<span class="number">1</span>) <span class="comment"># explanatory variable</span></span><br><span class="line">y = x*x*x+rand(<span class="number">40</span>,<span class="number">1</span>)/<span class="number">5</span> <span class="comment"># depentend variable</span></span><br></pre></td></tr></table></figure></p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 训练</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line">linreg = LinearRegression()</span><br><span class="line">linreg.fit(x,y)</span><br></pre></td></tr></table></figure></p>
<pre><code>LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)
</code></pre>
<h3>评估：结果可视化</h3>
<p>把拟合线和实际数据点画在同一幅图上来评估结果：</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> linspace, matrix</span><br><span class="line">xx = linspace(<span class="number">0</span>,<span class="number">1</span>,<span class="number">40</span>) <span class="comment"># 产生0-1之间40个数，步长相等</span></span><br><span class="line">plot(x,y,<span class="string">'go'</span>)</span><br><span class="line">plot(xx,linreg.predict(matrix(xx).T),<span class="string">'--r'</span>)</span><br><span class="line">show()</span><br></pre></td></tr></table></figure></p>
<p><img src="/output_47_0.png" alt="png"></p>
<h3>评估：均方误差</h3>
<p>使用均方误差来量化模型和原始数据的拟合度：
该指标度量了预期的拟合线和真实数据之间的距离平方。当拟合线很完美时该值为0。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="keyword">print</span> mean_squared_error(linreg.predict(x),y)</span><br></pre></td></tr></table></figure></p>
<pre><code>0.00967543840746
</code></pre>
<h2>降维</h2>
<p>最著名的降维技术之一就是主成分分析（PCA）。该技术把数据变量转换为等量或更少的不相关变量，称为主成分（PCs）。</p>
<p>下面我们实例化了一个PCA对象，用于计算前两个主成分。</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA(n_components=<span class="number">2</span>)</span><br></pre></td></tr></table></figure></p>
<h3>转换计算如下：</h3>
<p>将原来的四维数据转换为两维</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pcad = pca.fit_transform(data)</span><br></pre></td></tr></table></figure></p>
<h3>结果可视化</h3>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## 原来的数据</span></span><br><span class="line">subplot(<span class="number">121</span>)  </span><br><span class="line">plot(data[target==<span class="string">'setosa'</span>,<span class="number">0</span>],data[target==<span class="string">'setosa'</span>,<span class="number">2</span>],<span class="string">'bo'</span>)</span><br><span class="line">plot(data[target==<span class="string">'versicolor'</span>,<span class="number">0</span>],data[target==<span class="string">'versicolor'</span>,<span class="number">2</span>],<span class="string">'ro'</span>)</span><br><span class="line">plot(data[target==<span class="string">'virginica'</span>,<span class="number">0</span>],data[target==<span class="string">'virginica'</span>,<span class="number">2</span>],<span class="string">'go'</span>)</span><br><span class="line"><span class="comment">## 降维后的数据</span></span><br><span class="line">subplot(<span class="number">122</span>)</span><br><span class="line">plot(pcad[target==<span class="string">'setosa'</span>,<span class="number">0</span>],pcad[target==<span class="string">'setosa'</span>,<span class="number">1</span>],<span class="string">'bo'</span>)</span><br><span class="line">plot(pcad[target==<span class="string">'versicolor'</span>,<span class="number">0</span>],pcad[target==<span class="string">'versicolor'</span>,<span class="number">1</span>],<span class="string">'ro'</span>)</span><br><span class="line">plot(pcad[target==<span class="string">'virginica'</span>,<span class="number">0</span>],pcad[target==<span class="string">'virginica'</span>,<span class="number">1</span>],<span class="string">'go'</span>)</span><br></pre></td></tr></table></figure></p>
<pre><code>[&lt;matplotlib.lines.Line2D at 0x108e44210&gt;]
</code></pre>
<p><img src="/output_55_1.png" alt="png"></p>
<p>两个图有些相似，降维后，变色鸢尾（红色的）和维吉尼亚鸢尾（绿色的）的间隔更清晰了。</p>
<h3>信息量判断</h3>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#方差比判断PCs包含的信息量：</span></span><br><span class="line"><span class="keyword">print</span> pca.explained_variance_ratio_</span><br></pre></td></tr></table></figure></p>
<pre><code>[ 0.92461621  0.05301557]
</code></pre>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出在转化过程中丢失的信息量</span></span><br><span class="line"><span class="keyword">print</span> <span class="number">1</span>-sum(pca.explained_variance_ratio_)</span><br></pre></td></tr></table></figure></p>
<pre><code>0.0223682249752
</code></pre>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通过改变主成分的数值来计算我们能够覆盖多少信息量</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">5</span>):</span><br><span class="line">    pca = PCA(n_components=i)</span><br><span class="line">    pca.fit(data)</span><br><span class="line">    <span class="keyword">print</span> sum(pca.explained_variance_ratio_) * <span class="number">100</span>,<span class="string">'%'</span></span><br></pre></td></tr></table></figure></p>
<pre><code>92.4616207174 %
97.7631775025 %
99.481691455 %
100.0 %
</code></pre>
<p>可以发现PCs用得越多，信息覆盖就越全，</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure></p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info">Wente Guo</span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info">http://chnwentao.com/2016/05/数据挖掘/DM-总结-利用Python练习数据挖掘实例/</span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/算法/">算法</a><a class="post-meta__tags" href="/tags/python/">python</a><a class="post-meta__tags" href="/tags/机器学习/">机器学习</a><a class="post-meta__tags" href="/tags/ML/">ML</a><a class="post-meta__tags" href="/tags/数据挖掘/">数据挖掘</a><a class="post-meta__tags" href="/tags/DM/">DM</a></div></article><nav id="pagination"><div class="prev-post pull-left"><a href="/2016/05/机器学习/ML-总结-python花式读取数据/"><i class="fa fa-chevron-left">  </i><span>python花式读取数据</span></a></div><div class="next-post pull-right"><a href="/2016/05/机器学习/ML-总结-几种常见的回归算法/"><span>几种常见的回归算法</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">©2015 - 2017 By Wente Guo</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.3"></script><script src="/js/fancybox.js?version=1.3"></script><script src="/js/sidebar.js?version=1.3"></script><script src="/js/copy.js?version=1.3"></script><script src="/js/fireworks.js?version=1.3"></script><script src="/js/transition.js?version=1.3"></script><script src="/js/scroll.js?version=1.3"></script><script src="/js/head.js?version=1.3"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>