<!DOCTYPE html><html lang=""><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>几种常见的回归算法 | CS Learning of CHNwentao</title><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css?version=1.3"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?js?c4c99183a6a02c53d4760ae1995c3b38";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL = { 
  root: '/',
  algolia: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  localSearch: {"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}"},"path":"search.xml"}
} </script><link rel="icon" href="/favicon.ico"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#什么是回归分析？"><span class="toc-number">1.</span> <span class="toc-text">什么是回归分析？</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Linear-Regression线性回归"><span class="toc-number">2.</span> <span class="toc-text">Linear Regression线性回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#如何获得最佳拟合线（a和b的值）？"><span class="toc-number">2.1.</span> <span class="toc-text">如何获得最佳拟合线（a和b的值）？</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#要点："><span class="toc-number">2.2.</span> <span class="toc-text">要点：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Logistic-Regression逻辑回归"><span class="toc-number">3.</span> <span class="toc-text">Logistic Regression逻辑回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#要点：-1"><span class="toc-number">3.1.</span> <span class="toc-text">要点：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Polynomial-Regression多项式回归"><span class="toc-number">4.</span> <span class="toc-text">Polynomial Regression多项式回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Stepwise-Regression逐步回归"><span class="toc-number">5.</span> <span class="toc-text">Stepwise Regression逐步回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#实现方法原理是："><span class="toc-number">6.</span> <span class="toc-text">实现方法原理是：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Ridge-Regression岭回归"><span class="toc-number">7.</span> <span class="toc-text">Ridge Regression岭回归</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#要点：-2"><span class="toc-number">8.</span> <span class="toc-text">要点：</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Lasso-Regression套索回归"><span class="toc-number">9.</span> <span class="toc-text">Lasso Regression套索回归</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#要点：-3"><span class="toc-number">9.1.</span> <span class="toc-text">要点：</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#ElasticNet回归"><span class="toc-number">10.</span> <span class="toc-text">ElasticNet回归</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="http://7xiuu0.com1.z0.glb.clouddn.com/17-11-23/22222257.jpg"></div><div class="author-info__name text-center">Wente Guo</div><div class="author-info__description text-center">一个学习的博客网站，专注于计算机科学、机器学习、数据挖掘等的 技术分享、相关咨询及经验总结</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">134</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">184</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">19</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container" style="background-image: url(true);"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">CS Learning of CHNwentao</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span></div><div id="post-info"><div id="post-title">几种常见的回归算法</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2016-05-18</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"> </i><a class="post-meta__categories" href="/categories/机器学习/">机器学习</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1,712</span><span class="post-meta__separator">|</span><span>Reading time: 5 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div id="post-content"><p>重点总结了应该掌握的线性回归、逻辑回归、多项式回归、逐步回归、岭回归、套索回归、ElasticNet回归等七种最常用的回归技术及其关键要素，最后介绍了选择正确的回归模型的关键因素。<a id="more"></a></p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><h2 id="什么是回归分析？"><a href="#什么是回归分析？" class="headerlink" title="什么是回归分析？"></a>什么是回归分析？</h2><p>回归分析是一种预测性的建模技术，它研究的是因变量（目标）和自变量（预测器）之间的关系。回归分析通常用于预测分析，时间序列模型以及发现变量之间的因果关系。例如，司机的鲁莽驾驶与道路交通事故数量之间的关系，最好的研究方法就是回归。</p>
<h2 id="Linear-Regression线性回归"><a href="#Linear-Regression线性回归" class="headerlink" title="Linear Regression线性回归"></a>Linear Regression线性回归</h2><p>线性回归通常是人们在学习预测模型时首选的技术之一。在这种技术中，因变量是连续的，自变量可以是连续的也可以是离散的，回归线的性质是线性的。</p>
<p>线性回归使用最佳的拟合直线（也就是回归线）在因变量（Y）和一个或多个自变量（X）之间建立一种关系。<br>用一个方程式来表示它，即Y=a+b*X + e，其中a表示截距，b表示直线的斜率，e是误差项。这个方程可以根据给定的预测变量（s）来预测目标变量的值。</p>
<h3 id="如何获得最佳拟合线（a和b的值）？"><a href="#如何获得最佳拟合线（a和b的值）？" class="headerlink" title="如何获得最佳拟合线（a和b的值）？"></a>如何获得最佳拟合线（a和b的值）？</h3><p>使用最小二乘法,最小二乘法也是用于拟合回归线最常用的方法.</p>
<p>最小二乘法对于观测数据，它通过最小化每个数据点到线的垂直偏差平方和来计算最佳拟合线。因为在相加时，偏差先平方，所以正值和负值没有抵消。</p>
<h3 id="要点："><a href="#要点：" class="headerlink" title="要点："></a>要点：</h3><ul>
<li>自变量与因变量之间必须有线性关系</li>
<li>多元回归存在多重共线性，自相关性和异方差性。</li>
<li>线性回归对异常值非常敏感。它会严重影响回归线，最终影响预测值。</li>
<li>多重共线性会增加系数估计值的方差，使得在模型轻微变化下，估计非常敏感。结果就是系数估计值不稳定</li>
<li>在多个自变量的情况下，我们可以使用向前选择法，向后剔除法和逐步筛选法来选择最重要的自变量。</li>
</ul>
<h2 id="Logistic-Regression逻辑回归"><a href="#Logistic-Regression逻辑回归" class="headerlink" title="Logistic Regression逻辑回归"></a>Logistic Regression逻辑回归</h2><p>逻辑回归是用来计算“事件=Success”和“事件=Failure”的概率。<br>当因变量的类型属于 <strong>二元（1 / 0，真/假，是/否）变量</strong> 时，我们就应该使用逻辑回归。</p>
<h3 id="要点：-1"><a href="#要点：-1" class="headerlink" title="要点："></a>要点：</h3><ul>
<li>它广泛的用于分类问题。</li>
<li>辑回归不要求自变量和因变量是线性关系。它可以处理各种类型的关系，因为它对预测的相对风险指数OR使用了一个非线性的log转换。</li>
<li>为了避免过拟合和欠拟合，我们应该包括所有重要的变量。有一个很好的方法来确保这种情况，就是使用逐步筛选方法来估计逻辑回归。</li>
<li>它需要 <strong>大的样本量</strong>，因为在样本数量较少的情况下，极大似然估计的效果比普通的最小二乘法差。</li>
<li>自变量不应该相互关联的，即不具有多重共线性。然而，在分析和建模中，我们可以选择包含分类变量相互作用的影响。</li>
<li>如果因变量的值是定序变量，则称它为 <strong>序逻辑回归</strong>。</li>
<li>如果因变量是多类的话，则称它为 <strong>多元逻辑回归</strong>。</li>
</ul>
<h2 id="Polynomial-Regression多项式回归"><a href="#Polynomial-Regression多项式回归" class="headerlink" title="Polynomial Regression多项式回归"></a>Polynomial Regression多项式回归</h2><p>对于一个回归方程，如果自变量的指数大于1，那么它就是多项式回归方程。<br>多项式回归中最佳拟合线不是直线。而是一个用于拟合数据点的 <strong>曲线</strong>。</p>
<h2 id="Stepwise-Regression逐步回归"><a href="#Stepwise-Regression逐步回归" class="headerlink" title="Stepwise Regression逐步回归"></a>Stepwise Regression逐步回归</h2><p>在处理多个自变量时，我们可以使用这种形式的回归。在这种技术中，自变量的选择是在一个自动的过程中完成的，其中包括非人为操作。<br>算法的目标使用最少的预测变量数来最大化预测能力。这也是<strong>处理高维数据集</strong> 的方法之一。</p>
<h2 id="实现方法原理是："><a href="#实现方法原理是：" class="headerlink" title="实现方法原理是："></a>实现方法原理是：</h2><p>通过观察统计的值，如R-square，t-stats和AIC指标，来识别重要的变量。逐步回归通过同时添加/删除基于指定标准的协变量来拟合模型。下面列出了一些最常用的逐步回归方法：</p>
<ul>
<li>标准逐步回归法做两件事情。即增加和删除每个步骤所需的预测。</li>
<li>向前选择法从模型中最显著的预测开始，然后为每一步添加变量。</li>
<li>向后剔除法与模型的所有预测同时开始，然后在每一步消除最小显着性的变量。</li>
</ul>
<h2 id="Ridge-Regression岭回归"><a href="#Ridge-Regression岭回归" class="headerlink" title="Ridge Regression岭回归"></a>Ridge Regression岭回归</h2><p>岭回归分析是一种用于存在多重共线性（自变量高度相关）数据的技术。在多重共线性情况下，尽管最小二乘法（OLS）对每个变量很公平，但它们的差异很大，使得观测值偏移并远离真实值。岭回归通过给回归估计上增加一个偏差度，来降低标准误差。<br>#</p>
<h2 id="要点：-2"><a href="#要点：-2" class="headerlink" title="要点："></a>要点：</h2><ul>
<li>除常数项以外，这种回归的假设与最小二乘回归类似；</li>
<li>它收缩了相关系数的值，但没有达到零，这表明它<strong>没有特征选择功能</strong></li>
<li>这是一个正则化方法，并且使用的是L2正则化。</li>
</ul>
<h2 id="Lasso-Regression套索回归"><a href="#Lasso-Regression套索回归" class="headerlink" title="Lasso Regression套索回归"></a>Lasso Regression套索回归</h2><p>类似于岭回归，Lasso （Least Absolute Shrinkage and Selection Operator）也会惩罚回归系数的绝对值大小。此外，它能够减少变化程度并提高线性回归模型的精度。</p>
<p>如果预测的一组变量是高度相关的，Lasso 会选出其中一个变量并且将其它的收缩为零。</p>
<h3 id="要点：-3"><a href="#要点：-3" class="headerlink" title="要点："></a>要点：</h3><ul>
<li>除常数项以外，这种回归的假设与最小二乘回归类似；</li>
<li>它收缩系数接近零（等于零），这确实 <strong>有助于特征选择</strong>；</li>
<li>这是一个正则化方法，使用的是L1正则化；</li>
</ul>
<h2 id="ElasticNet回归"><a href="#ElasticNet回归" class="headerlink" title="ElasticNet回归"></a>ElasticNet回归</h2><p>ElasticNet是Lasso和Ridge回归技术的混合体。它使用L1来训练并且L2优先作为正则化矩阵。当有多个相关的特征时，ElasticNet是很有用的。Lasso 会随机挑选他们其中的一个，而ElasticNet则会选择两个。</p>
<p>Lasso和Ridge之间的实际的优点是，它允许ElasticNet继承循环状态下Ridge的一些 <strong>稳定性</strong>。</p>
<p>要点：</p>
<ul>
<li>在高度相关变量的情况下，它会产生群体效应；</li>
<li>选择变量的数目没有限制；</li>
<li>它可以承受双重收缩。</li>
</ul>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info">Wente Guo</span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info">http://chnwentao.com/2016/05/机器学习/ML-总结-几种常见的回归算法/</span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/算法/">算法</a><a class="post-meta__tags" href="/tags/机器学习/">机器学习</a><a class="post-meta__tags" href="/tags/ML/">ML</a><a class="post-meta__tags" href="/tags/回归/">回归</a></div></article><nav id="pagination"><div class="prev-post pull-left"><a href="/2016/05/数据挖掘/DM-总结-利用Python练习数据挖掘实例/"><i class="fa fa-chevron-left">  </i><span>利用Python练习数据挖掘实例</span></a></div><div class="next-post pull-right"><a href="/2016/05/机器学习/ML-总结-回归算法的理解/"><span>机器学习中回归类算法的理解</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">©2015 - 2017 By Wente Guo</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a><span class="footer-separator">|</span><span>Hosted by </span><a href="https://pages.coding.me"><span>Coding Pages</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.3"></script><script src="/js/fancybox.js?version=1.3"></script><script src="/js/sidebar.js?version=1.3"></script><script src="/js/copy.js?version=1.3"></script><script src="/js/fireworks.js?version=1.3"></script><script src="/js/transition.js?version=1.3"></script><script src="/js/scroll.js?version=1.3"></script><script src="/js/head.js?version=1.3"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>