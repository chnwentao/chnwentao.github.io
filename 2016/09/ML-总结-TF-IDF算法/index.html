<!DOCTYPE html><html lang="en"><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>TF-IDF介绍及python实例 | CS Learning of CHNwentao</title><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css?version=1.3"><script>var GLOBAL = { 
  root: '/',
  algolia: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  localSearch: {"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}"},"path":"search.xml"}
} </script><link rel="icon" href="/favicon.ico"></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#TF-IDF介绍"><span class="toc-number">1.</span> <span class="toc-text">TF-IDF介绍</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#词频（Term-Frequency，TF）"><span class="toc-number">1.1.</span> <span class="toc-text">词频（Term Frequency，TF）</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#逆向文件频率（Inverse-Document-Frequency，IDF）"><span class="toc-number">1.2.</span> <span class="toc-text">逆向文件频率（Inverse Document Frequency，IDF）</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#应用"><span class="toc-number">2.</span> <span class="toc-text">应用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TFIDF的理论依据及不足"><span class="toc-number">3.</span> <span class="toc-text">TFIDF的理论依据及不足</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#Python中计算TF-IDF-实例"><span class="toc-number">4.</span> <span class="toc-text">Python中计算TF-IDF 实例</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#环境"><span class="toc-number">4.1.</span> <span class="toc-text">环境</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#计算TF-IDF"><span class="toc-number">4.2.</span> <span class="toc-text">计算TF-IDF</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="http://7xiuu0.com1.z0.glb.clouddn.com/17-11-23/22222257.jpg"></div><div class="author-info__name text-center">Wente Guo</div><div class="author-info__description text-center">一个学习的博客网站，专注于计算机科学、机器学习、数据挖掘等的 技术分享、相关咨询及经验总结</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">129</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">180</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">18</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container" style="background-image: url(true);"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">CS Learning of CHNwentao</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span></div><div id="post-info"><div id="post-title">TF-IDF介绍及python实例</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2016-09-01</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"> </i><a class="post-meta__categories" href="/categories/机器学习/">机器学习</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">1,552</span><span class="post-meta__separator">|</span><span>Reading time: 6 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div id="post-content"><h2 id="TF-IDF介绍"><a href="#TF-IDF介绍" class="headerlink" title="TF-IDF介绍"></a>TF-IDF介绍</h2><p>TF-IDF（Term Frequency–Inverse Document Frequency）是一种用于资讯检索与文本挖掘的常用加权技术。TF-IDF是一种统计方法，用以评估一个字词对于一个文件集或一个语料库中的其中一份文件的重要程度。字词的重要性随着它在文件中出现的次数成正比增加，但同时会随着它在语料库中出现的频率成反比下降。TF-IDF加权的各种形式常被搜索引擎应用，作为文件与用户查询之间相关程度的度量或评级。<a id="more"></a><br>TF-IDF的主要思想是：如果某个词或短语在一篇文章中出现的频率TF高，并且在其他文章中很少出现，则认为此词或者短语具有很好的类别区分能力，适合用来分类。TF-IDF实际上是：<code>TF * IDF</code>。</p>
<h3 id="词频（Term-Frequency，TF）"><a href="#词频（Term-Frequency，TF）" class="headerlink" title="词频（Term Frequency，TF）"></a>词频（Term Frequency，TF）</h3><p>词频指的是某一个给定的词语在该文件中出现的频率。即词w在文档d中出现的次数count(w, d)和文档d中总词数size(d)的比值。<br><code>tf(w,d) = count(w, d) / size(d)</code><br>这个数字是对词数(term count)的归一化，以防止它偏向长的文件。（同一个词语在长文件里可能会比短文件有更高的词数，而不管该词语重要与否。）</p>
<h3 id="逆向文件频率（Inverse-Document-Frequency，IDF）"><a href="#逆向文件频率（Inverse-Document-Frequency，IDF）" class="headerlink" title="逆向文件频率（Inverse Document Frequency，IDF）"></a>逆向文件频率（Inverse Document Frequency，IDF）</h3><p>逆向文件频率是一个词语普遍重要性的度量。某一特定词语的IDF，可以由总文件数目除以包含该词语之文件的数目，再将得到的商取对数得到。即文档总数n与词w所出现文件数docs(w, D)比值的对数。　　<br><code>idf = log(n / docs(w, D))</code></p>
<p>某一特定文件内的高词语频率，以及该词语在整个文件集合中的低文件频率，可以产生出高权重的TF-IDF。因此，TF-IDF倾向于过滤掉常见的词语，保留重要的词语。</p>
<h2 id="应用"><a href="#应用" class="headerlink" title="应用"></a>应用</h2><p>TF-IDF权重计算方法经常会和余弦相似性（cosine similarity）一同使用于向量空间模型中，用以判断两份文件之间的相似性。</p>
<h2 id="TFIDF的理论依据及不足"><a href="#TFIDF的理论依据及不足" class="headerlink" title="TFIDF的理论依据及不足"></a>TFIDF的理论依据及不足</h2><p>TFIDF算法是建立在这样一个假设之上的：对区别文档最有意义的词语应该是那些在文档中出现频率高，而在整个文档集合的其他文档中出现频率少的词语，所以如果特征空间坐标系取TF词频作为测度，就可以体现同类文本的特点。另外考虑到单词区别不同类别的能力，TFIDF法认为一个单词出现的文本频数越小，它区别不同类别文本的能力就越大。因此引入了逆文本频度IDF的概念，以TF和IDF的乘积作为特征空间坐标系的取值测度，并用它完成对权值TF的调整，调整权值的目的在于突出重要单词，抑制次要单词。但是在本质上IDF是一种试图抑制噪声的加权，并且单纯地认为文本频率小的单词就越重要，文本频率大的单词就越无用，显然这并不是完全正确的。IDF的简单结构并不能有效地反映单词的重要程度和特征词的分布情况，使其无法很好地完成对权值调整的功能，所以TFIDF法的精度并不是很高。</p>
<p>此外，在TFIDF算法中并没有体现出单词的 <strong>位置信息</strong>，对于Web文档而言，权重的计算方法应该体现出HTML的结构特征。特征词在不同的标记符中对文章内容的反映程度不同，其权重的计算方法也应不同。因此应该对于处于网页不同位置的特征词分别赋予不同的系数，然后乘以特征词的词频，以提高文本表示的效果。</p>
<h2 id="Python中计算TF-IDF-实例"><a href="#Python中计算TF-IDF-实例" class="headerlink" title="Python中计算TF-IDF 实例"></a>Python中计算TF-IDF 实例</h2><h3 id="环境"><a href="#环境" class="headerlink" title="环境"></a>环境</h3><p>scikit-learn包</p>
<p>jieba 分词包</p>
<h3 id="计算TF-IDF"><a href="#计算TF-IDF" class="headerlink" title="计算TF-IDF"></a>计算TF-IDF</h3><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> jieba</span><br><span class="line"><span class="keyword">import</span> jieba.posseg <span class="keyword">as</span> pseg</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> string</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> feature_extraction</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> TfidfTransformer</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_extraction.text <span class="keyword">import</span> CountVectorizer</span><br><span class="line">reload(sys)</span><br><span class="line">sys.setdefaultencoding(<span class="string">'utf8'</span>)</span><br><span class="line"><span class="comment">#获取文件列表（该目录下放着100份文档）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">getFilelist</span><span class="params">(argv)</span> :</span></span><br><span class="line">    path = argv[<span class="number">1</span>]</span><br><span class="line">    filelist = []</span><br><span class="line">    files = os.listdir(path)</span><br><span class="line">    <span class="keyword">for</span> f <span class="keyword">in</span> files :</span><br><span class="line">        <span class="keyword">if</span>(f[<span class="number">0</span>] == <span class="string">'.'</span>) :</span><br><span class="line">            <span class="keyword">pass</span></span><br><span class="line">        <span class="keyword">else</span> :</span><br><span class="line">            filelist.append(f)</span><br><span class="line">    <span class="keyword">return</span> filelist,path</span><br><span class="line"><span class="comment">#对文档进行分词处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fenci</span><span class="params">(argv,path)</span> :</span></span><br><span class="line">    <span class="comment">#保存分词结果的目录</span></span><br><span class="line">    sFilePath = <span class="string">'./segfile'</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(sFilePath) :</span><br><span class="line">        os.mkdir(sFilePath)</span><br><span class="line">    <span class="comment">#读取文档</span></span><br><span class="line">    filename = argv</span><br><span class="line">    f = open(path+filename,<span class="string">'r+'</span>)</span><br><span class="line">    file_list = f.read()</span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line">    <span class="comment">#对文档进行分词处理，采用默认模式</span></span><br><span class="line">    seg_list = jieba.cut(file_list,cut_all=<span class="keyword">True</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#对空格，换行符进行处理</span></span><br><span class="line">    result = []</span><br><span class="line">    <span class="keyword">for</span> seg <span class="keyword">in</span> seg_list :</span><br><span class="line">　　　　 seg = <span class="string">''</span>.join(seg.split())</span><br><span class="line">        <span class="keyword">if</span> (seg != <span class="string">''</span> <span class="keyword">and</span> seg != <span class="string">"\n"</span> <span class="keyword">and</span> seg != <span class="string">"\n\n"</span>) :</span><br><span class="line">            result.append(seg)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#将分词后的结果用空格隔开，保存至本地。比如"我来到北京清华大学"，分词结果写入为："我 来到 北京 清华大学"</span></span><br><span class="line">    f = open(sFilePath+<span class="string">"/"</span>+filename+<span class="string">"-seg.txt"</span>,<span class="string">"w+"</span>)</span><br><span class="line">    f.write(<span class="string">' '</span>.join(result))</span><br><span class="line">    f.close()</span><br><span class="line"></span><br><span class="line"><span class="comment">#读取100份已分词好的文档，进行TF-IDF计算</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">Tfidf</span><span class="params">(filelist)</span> :</span></span><br><span class="line">　　path = <span class="string">'./segfile／'</span></span><br><span class="line">    corpus = []  <span class="comment">#存取100份文档的分词结果</span></span><br><span class="line">    <span class="keyword">for</span> ff <span class="keyword">in</span> filelist :</span><br><span class="line">        fname = path + ff</span><br><span class="line">        f = open(fname,<span class="string">'r+'</span>)</span><br><span class="line">        content = f.read()</span><br><span class="line">        f.close()</span><br><span class="line">        corpus.append(content)    </span><br><span class="line"></span><br><span class="line">    vectorizer = CountVectorizer()    </span><br><span class="line">    transformer = TfidfTransformer()</span><br><span class="line">    tfidf = transformer.fit_transform(vectorizer.fit_transform(corpus))</span><br><span class="line"></span><br><span class="line">    word = vectorizer.get_feature_names() <span class="comment">#所有文本的关键字</span></span><br><span class="line">    weight = tfidf.toarray()              <span class="comment">#对应的tfidf矩阵</span></span><br><span class="line"></span><br><span class="line">    sFilePath = <span class="string">'./tfidffile'</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(sFilePath) :</span><br><span class="line">        os.mkdir(sFilePath)</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 这里将每份文档词语的TF-IDF写入tfidffile文件夹中保存</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> range(len(weight)) :</span><br><span class="line">　　　　 <span class="keyword">print</span> <span class="string">u"--------Writing all the tf-idf in the"</span>,i,<span class="string">u" file into "</span>,sFilePath+<span class="string">'/'</span>+string.zfill(i,<span class="number">5</span>)+<span class="string">'.txt'</span>,<span class="string">"--------"</span></span><br><span class="line">        f = open(sFilePath+<span class="string">'/'</span>+string.zfill(i,<span class="number">5</span>)+<span class="string">'.txt'</span>,<span class="string">'w+'</span>)</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> range(len(word)) :</span><br><span class="line">            f.write(word[j]+<span class="string">"    "</span>+str(weight[i][j])+<span class="string">"\n"</span>)</span><br><span class="line">        f.close()</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span> :</span><br><span class="line">    (allfile,path) = getFilelist(sys.argv)</span><br><span class="line">　　<span class="keyword">for</span> ff <span class="keyword">in</span> allfile :</span><br><span class="line">        <span class="keyword">print</span> <span class="string">"Using jieba on "</span>+ff</span><br><span class="line">        fenci(ff,path)</span><br><span class="line"></span><br><span class="line">    Tfidf(allfile)</span><br></pre></td></tr></table></figure>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info">Wente Guo</span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info">http://chnwentao.com/2016/09/ML-总结-TF-IDF算法/</span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/机器学习/">机器学习</a><a class="post-meta__tags" href="/tags/TF-IDF/">TF-IDF</a></div></article><nav id="pagination"><div class="prev-post pull-left"><a href="/2016/09/DM-xgboost-训练数据格式总结/"><i class="fa fa-chevron-left">  </i><span>xgboost 训练数据格式总结</span></a></div><div class="next-post pull-right"><a href="/2016/07/ML-总结-用一个框架解决所有机器学习难题的学习/"><span>用一个流程解决所有机器学习难题的学习</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">©2015 - 2017 By Wente Guo</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.3"></script><script src="/js/fancybox.js?version=1.3"></script><script src="/js/sidebar.js?version=1.3"></script><script src="/js/copy.js?version=1.3"></script><script src="/js/fireworks.js?version=1.3"></script><script src="/js/transition.js?version=1.3"></script><script src="/js/scroll.js?version=1.3"></script><script src="/js/head.js?version=1.3"></script></body></html>