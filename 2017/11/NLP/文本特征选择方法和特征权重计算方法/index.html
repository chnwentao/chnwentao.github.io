<!DOCTYPE html><html lang=""><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>文本特征选择方法和特征权重计算方法 | CS Learning of CHNwentao</title><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css?version=1.3"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?js?c4c99183a6a02c53d4760ae1995c3b38";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL = { 
  root: '/',
  algolia: undefined,
  copy: {
    success: 'Copy successfully',
    error: 'Copy error',
    noSupport: 'The browser does not support'
  },
  localSearch: {"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}"},"path":"search.xml"}
} </script><link rel="icon" href="/favicon.ico"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="Toggle article">Toggle site</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">Catalog</div><div class="sidebar-toc__progress"><span class="progress-notice">You've read</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#基础概念"><span class="toc-number">1.</span> <span class="toc-text">基础概念</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#什么是文本分类："><span class="toc-number">1.1.</span> <span class="toc-text">什么是文本分类：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#什么是文本表示："><span class="toc-number">1.2.</span> <span class="toc-text">什么是文本表示：</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#什么是理想的文本表示（特征）"><span class="toc-number">1.3.</span> <span class="toc-text">什么是理想的文本表示（特征）?</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#特征征选择-amp-特征权重计算"><span class="toc-number">1.4.</span> <span class="toc-text">特征征选择 & 特征权重计算</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#特征选择方法"><span class="toc-number">2.</span> <span class="toc-text">特征选择方法</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#词频方法（Word-Frequency）："><span class="toc-number">3.</span> <span class="toc-text">词频方法（Word Frequency）：</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#文档频数-DF"><span class="toc-number">3.1.</span> <span class="toc-text">文档频数 DF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#信息增益方法-Information-Gain"><span class="toc-number">3.2.</span> <span class="toc-text">信息增益方法 (Information Gain)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#卡方统计量-CHI"><span class="toc-number">3.3.</span> <span class="toc-text">卡方统计量 CHI</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#互信息-Mutual-Information"><span class="toc-number">3.4.</span> <span class="toc-text">互信息(Mutual Information)</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#比较"><span class="toc-number">3.5.</span> <span class="toc-text">比较</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#特征权重计算"><span class="toc-number">4.</span> <span class="toc-text">特征权重计算</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#布尔特征，"><span class="toc-number">5.</span> <span class="toc-text">布尔特征，</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#TF"><span class="toc-number">6.</span> <span class="toc-text">TF</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#TF-IDF"><span class="toc-number">6.1.</span> <span class="toc-text">TF-IDF</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TF-IDF-于多分类"><span class="toc-number">6.2.</span> <span class="toc-text">TF-IDF 于多分类</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#TFC-amp-ITC-amp-TF-IWF"><span class="toc-number">6.3.</span> <span class="toc-text">TFC & ITC & TF-IWF</span></a></li></ol></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="http://7xiuu0.com1.z0.glb.clouddn.com/17-11-23/22222257.jpg"></div><div class="author-info__name text-center">Wente Guo</div><div class="author-info__description text-center">一个学习的博客网站，专注于计算机科学、机器学习、数据挖掘等的 技术分享、相关咨询及经验总结</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">Articles</span><span class="pull-right">144</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">Tags</span><span class="pull-right">192</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">Categories</span><span class="pull-right">20</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container" style="background-image: url(true);"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">CS Learning of CHNwentao</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span></div><div id="post-info"><div id="post-title">文本特征选择方法和特征权重计算方法</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2017-11-26</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"> </i><a class="post-meta__categories" href="/categories/NLP/">NLP</a><div class="post-meta-wordcount"><span>Word count: </span><span class="word-count">2,397</span><span class="post-meta__separator">|</span><span>Reading time: 7 min</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div id="post-content"><p>介绍了一下文本特征 提取的常用的方法，实际上这个是2月份写的博客，但是当时放在工作总结了，没有单独提出去。</p>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><a id="more"></a>
<h2 id="基础概念"><a href="#基础概念" class="headerlink" title="基础概念"></a>基础概念</h2><h3 id="什么是文本分类："><a href="#什么是文本分类：" class="headerlink" title="什么是文本分类："></a>什么是文本分类：</h3><p>再预定的分类体系下，根据文本的特征，将给定文本与一个或多个类别相关联。</p>
<h3 id="什么是文本表示："><a href="#什么是文本表示：" class="headerlink" title="什么是文本表示："></a>什么是文本表示：</h3><p>将文本翻译成机器可以理解或处理的形式. 也就是从 文本 —&gt; 特征集合</p>
<h3 id="什么是理想的文本表示（特征）"><a href="#什么是理想的文本表示（特征）" class="headerlink" title="什么是理想的文本表示（特征）?"></a>什么是理想的文本表示（特征）?</h3><ul>
<li><p>可以真实的反应文档的内容（向量空间就丢失了语序，和上下文 ）</p>
</li>
<li><p>对不同文档有区分能力 (ID 不能作为特征)</p>
</li>
</ul>
<h3 id="特征征选择-amp-特征权重计算"><a href="#特征征选择-amp-特征权重计算" class="headerlink" title="特征征选择 &amp; 特征权重计算"></a>特征征选择 &amp; 特征权重计算</h3><p>特征选择阶段的重要程度量化和将具体文本转化为向量时的特征权重量化。</p>
<p>权重代表的是差别</p>
<p>问题：</p>
<p>一个方法既适用于 特征征选择  也 特征权重计算 吗？ TF—IDF</p>
<h2 id="特征选择方法"><a href="#特征选择方法" class="headerlink" title="特征选择方法"></a>特征选择方法</h2><h2 id="词频方法（Word-Frequency）："><a href="#词频方法（Word-Frequency）：" class="headerlink" title="词频方法（Word Frequency）："></a>词频方法（Word Frequency）：</h2><p>词频是一个词在文档中出现的次数。</p>
<p>通过词频进行特征选择就是将词频小于某一闭值的词删除，从而降低特征空间的维数。</p>
<p>这个方法是基于这样一个假设，即出现频率小的词对过滤的影响也较小。但是在信息检索的研究中认为，有时频率小的词含有更多的信息。因此，在特征选择的过程中不宜简单地根据词频大幅度删词。</p>
<h3 id="文档频数-DF"><a href="#文档频数-DF" class="headerlink" title="文档频数 DF"></a>文档频数 DF</h3><p>原理：</p>
<ul>
<li><p>所用文档中统计包含某个特征的文档的频率，然后卡一个上下阀值。</p>
</li>
<li><p>频率太低的， 没有代表性 (稀有词要么不含有用信息,要么太少而不足以对分类产生影响,要么是噪音,所以可以删去。)</p>
</li>
<li><p>频率太高，没有区分度</p>
</li>
</ul>
<p>优点：</p>
<ul>
<li><p>速度快,它的时间复杂度和文本数量成线性关系,所以非常适合于超大规模文本数据集的特征选择。</p>
</li>
<li><p>文档频数还非常地高效,在有监督的特征选择应用中当删除90%单词的时候其性能与信息增益和x2 统计的性能还不相上下。</p>
</li>
</ul>
<p>缺点</p>
<ul>
<li><strong>稀有词</strong>可能在某一类文本中并不稀有,也可能包含着重要的判断信息,简单舍弃,可能影响分类器的精度。</li>
</ul>
<h3 id="信息增益方法-Information-Gain"><a href="#信息增益方法-Information-Gain" class="headerlink" title="信息增益方法 (Information Gain)"></a>信息增益方法 (Information Gain)</h3><p>基础：</p>
<p>信息量 ：一个变量可能的变化越多（反而跟变量具体的取值没有任何关系，只和值的种类多少以及发生概率有关），它携带的信息量就越大。</p>
<p>信息量越大， 不确定性越小。</p>
<p>不足：</p>
<p>信息息增益最大的问题还在于它只能考察特征对整个系统的贡献，而不能具体到某个类别上，这就使得它只适合用来做所谓“全局”的特征选择（指所有的类都使用相同的特征集合），而无法做“本地”的特征选择（每个类别有自己的特征集合，因为有的词，对这个类别很有区分度，对另一个类别则无足轻重）。</p>
<p>数据稀疏问题：许多信息增益高的特征出现频率比较低, 这样会引起数据稀疏问题，影响分类。</p>
<p>考虑了单词未发生的情况，对判断文本类别贡献不大，而且引入不必要的干扰，特别是在处理类分布和特征值分布高度不平衡的数据时选择精度下降。</p>
<p>参考：</p>
<p><a href="http://www.blogjava.net/zhenandaci/archive/2009/03/24/261701.html" target="_blank" rel="noopener">文本分类入门（十一）特征选择方法之信息增益</a></p>
<h3 id="卡方统计量-CHI"><a href="#卡方统计量-CHI" class="headerlink" title="卡方统计量 CHI"></a>卡方统计量 CHI</h3><p>基础：</p>
<p>开方检验:最基本的思想就是通过观察实际值与理论值的偏差来确定理论的正确与否。</p>
<p>零假设（null hypothesis） : 变量相互独立</p>
<p>临界值: 置信度为0.999的卡方临界值是10.83，也就是当卡方值&gt;10.83时，我们有99.9%的把握，t与c不独立，</p>
<p>原理：</p>
<p>一个词t（一个随机变量）与一个类别c（另一个随机变量）之间是否相互独立？如果独立，就可以说词t对类别c完全没有表征作用，即我们根本无法根据t出现与否来判断一篇文档是否属于c这个分类。</p>
<p>方法：</p>
<p>原假设: 词t与类别c不相关</p>
<p>备则假设：特征词与类别有着很高的关联度。</p>
<p>计算出的开方值越大，说明对原假设的偏离越大</p>
<p>多分类问题的情况：</p>
<p>对每个类别分别计算 CHI 取 max</p>
<p>不足：</p>
<p>低频词缺陷: 统计文档中是否出现词t，却不管t在该文档中出现了几次，这会使得他对低频词有所偏袒（因为它夸大了低频词的作用）。</p>
<p>特征提取步骤</p>
<p>1.1  统计样本集中文档总数（N）。</p>
<p>1.2  统计每个词的正文档出现频率（A）、负文档出现频率（B）、正文档不出现频率C 、负文档不出现频率D。</p>
<p>1.3 计算每个词的卡方值，公式如下： <code>N * （A*D - B*C）** 2 / ((A + C) * (A + B) * (C + D))</code></p>
<p>1.4  将每个词按卡方值从大到小排序，选取前k个词作为特征，k即特征维数。</p>
<h3 id="互信息-Mutual-Information"><a href="#互信息-Mutual-Information" class="headerlink" title="互信息(Mutual Information)"></a>互信息(Mutual Information)</h3><p>思想 ：</p>
<p>MI度量的是词的存在与否给类别c带来的信息量</p>
<p>互相信息越大，特征 t 和 类别 C 共现 程度越大。</p>
<p>果某个词项均匀的分布在各个类别，那么I(U;C)=0I(U;C)=0，当某词项总是出现在当前类别，而在其他类别中很少出现时，I(U;C)I(U;C)就会比较大。使用互信息能够保留具有信息含量的词项的同时，去掉那些没有信息含量的词项，从而提高正确率。</p>
<p>互信息计算的时间复杂度类似于信息增益，互信息的平均值就是信息增益。</p>
<p>不足：</p>
<p>没有考虑特征出现的频率，这样导致互信息评估函数不选择高频的有用词而有可能选择<strong>稀有词</strong>作为文本的最佳特征。</p>
<h3 id="比较"><a href="#比较" class="headerlink" title="比较"></a>比较</h3><p>英文文本中, 作为特征选择方法时，开方检验和信息增益的效果最佳（相同的分类算法，使用不同的特征选择算法来得到比较结果）；文档频率方法的性能同前两者大体相当，术语强度方法性能一般；互信息方法的性能最差 (Yiming Yang,Jan O Pedersen:A comparative Study on Feature Selection in Text Categorization, Proceedings of the Fourteenth International Conference on Machine Learning(ICML~97)，l997 )</p>
<h2 id="特征权重计算"><a href="#特征权重计算" class="headerlink" title="特征权重计算"></a>特征权重计算</h2><p>CHI、TFIDF既可以作为特征选取也可以作为权重计算的方法，不同之处在于TFIDF可以用于任意文本集合，而CHI则需要文本有分类标签的标记才能计算。</p>
<p>问题：</p>
<h2 id="布尔特征，"><a href="#布尔特征，" class="headerlink" title="布尔特征，"></a>布尔特征，</h2><p>最原始的， 出现于不出现 0 1</p>
<p>无法提前特征的作用程度</p>
<h2 id="TF"><a href="#TF" class="headerlink" title="TF"></a>TF</h2><p>特征词在这篇文档中的权重</p>
<p>无法提现低频特征的区分能力</p>
<h3 id="TF-IDF"><a href="#TF-IDF" class="headerlink" title="TF-IDF"></a>TF-IDF</h3><p>假设： 对区别文档最有意义的词语应该是那些在文档中出现频率高，而在整个文档集合的其他文档中出现频率少的词语，所以如果特征空间坐标系取TF词频作为测度，就可以体现同类文本的特点。</p>
<p>思想： 信息论，是在某个特定情况下，是词语分布与文档分布的KL距离。</p>
<p>TF: 指该特征词在这篇文档中的词频</p>
<p>IDF ： <code>idf (term) = log ( N / N_term + 1 )</code></p>
<p>如果一个词越常见，那么分母就越大，逆文档频率就越小越接近0。</p>
<p>分母之所以要加1，是为了避免分母为0（即所有文档都不包含该词）。log表示对得到的值取对数。</p>
<p>缺点:</p>
<ul>
<li><p>单纯以”词频”衡量一个词的重要性，不够全面，有时重要的词可能出现次数并不多</p>
</li>
<li><p>这种算法无法体现词的位置信息，出现位置靠前的词与出现位置靠后的词，都被视为重要性相同，这是不正确的。（一种解决方法是，对全文的第一段和每一段的第一句话，给予较大的权重。）</p>
</li>
</ul>
<h3 id="TF-IDF-于多分类"><a href="#TF-IDF-于多分类" class="headerlink" title="TF-IDF 于多分类"></a>TF-IDF 于多分类</h3><p>文本分类问题更复杂一些，因为除了词语、文档这两个原有的维度，又多了一个文档类别的维度。</p>
<p>改进的idf计算： <code>idf (term) = log ( N * M / N_term )</code></p>
<p>其中M是当前类别中包含词语term的文档数——这个要与最终计算的词语对该类别的权重成正比。</p>
<h3 id="TFC-amp-ITC-amp-TF-IWF"><a href="#TFC-amp-ITC-amp-TF-IWF" class="headerlink" title="TFC &amp; ITC &amp; TF-IWF"></a>TFC &amp; ITC &amp; TF-IWF</h3><p><code>TF*IDF</code>权重没有考虑不同的文本长度对词权重的影响。</p>
<p>TFC权重算法与 <code>TF*IDF</code> 权重算法的唯一不同在于前者对于文本长度进行了归一化处理。</p>
<p>ITC权重与TFC权重的不同在于使用词频的log值代替了词频，从而减少了词频对权重的影响</p>
<p>TF-IWF ：在TF-IDF算法的基础上，用特征项频率倒数的对数值IWF代替IDF，并且用IWF的平方平衡权重值对于特征项频率的倚重。</p>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">Author: </span><span class="post-copyright-info">Wente Guo</span></div><div class="post-copyright__type"><span class="post-copyright-meta">Link: </span><span class="post-copyright-info">http://chnwentao.com/2017/11/NLP/文本特征选择方法和特征权重计算方法/</span></div><div class="post-copyright__notice"><span class="post-copyright-meta">Copyright Notice: </span><span class="post-copyright-info">All articles in this blog are licensed under <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" target="_blank">CC BY-NC-SA 3.0</a> unless stating additionally.</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/NLP/">NLP</a><a class="post-meta__tags" href="/tags/文本处理/">文本处理</a></div></article><nav id="pagination"><div class="prev-post pull-left"><a href="/2017/11/文本分类/对于标签存在一定关联的情况下的文本分类的学习/"><i class="fa fa-chevron-left">  </i><span>对于标签存在一定关联的情况下的文本分类</span></a></div><div class="next-post pull-right"><a href="/2017/11/深度学习/CNN 在自然语言中的使用/"><span>CNN在自然语言处理中的使用</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">©2015 - 2017 By Wente Guo</div><div class="framework-info"><span>Driven - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>Theme - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a><span class="footer-separator">|</span><span>Hosted by </span><a href="https://pages.coding.me"><span>Coding Pages</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.3"></script><script src="/js/fancybox.js?version=1.3"></script><script src="/js/sidebar.js?version=1.3"></script><script src="/js/copy.js?version=1.3"></script><script src="/js/fireworks.js?version=1.3"></script><script src="/js/transition.js?version=1.3"></script><script src="/js/scroll.js?version=1.3"></script><script src="/js/head.js?version=1.3"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>