<!DOCTYPE html><html lang=""><head><meta charset="UTF-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"><title>基于 Gensim 的 Word2Vec 实践 | CS Learning of CHNwentao</title><link rel="stylesheet" href="/css/index.css"><link rel="stylesheet" href="https://cdn.staticfile.org/font-awesome/4.7.0/css/font-awesome.min.css?version=1.3"><script><!-- hexo-inject:begin --><!-- hexo-inject:end -->var _hmt = _hmt || [];
(function() {
  var hm = document.createElement("script");
  hm.src = "https://hm.baidu.com/hm.js?js?c4c99183a6a02c53d4760ae1995c3b38";
  var s = document.getElementsByTagName("script")[0]; 
  s.parentNode.insertBefore(hm, s);
})();</script><script>var GLOBAL = { 
  root: '/',
  algolia: undefined,
  copy: {
    success: '复制成功',
    error: '复制错误',
    noSupport: '浏览器不支持'
  },
  localSearch: {"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}"},"path":"search.xml"}
} </script><link rel="icon" href="/favicon.ico"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head><body><canvas class="fireworks"></canvas><i class="fa fa-arrow-right" id="toggle-sidebar" aria-hidden="true"></i><div id="sidebar"><div class="toggle-sidebar-info text-center"><span data-toggle="切换文章详情">切换站点概览</span><hr></div><div class="sidebar-toc"><div class="sidebar-toc__title">目录</div><div class="sidebar-toc__progress"><span class="progress-notice">你已经读了</span><span class="progress-num">0</span><span class="progress-percentage">%</span><div class="sidebar-toc__progress-bar"></div></div><div class="sidebar-toc__content"><ol class="toc"><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">1.</span> <span class="toc-text">模型原理</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">2.</span> <span class="toc-text">准备工作</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">2.1.</span> <span class="toc-text">安装</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">3.</span> <span class="toc-text">加载语料库</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">4.</span> <span class="toc-text">参数的设定</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">4.1.</span> <span class="toc-text">min_count</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">4.2.</span> <span class="toc-text">size</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">4.3.</span> <span class="toc-text">workers</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">5.</span> <span class="toc-text">存储和加载模型</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#undefined"><span class="toc-number">5.1.</span> <span class="toc-text">直接储存或加载由C生成的模型:</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">6.</span> <span class="toc-text">训练</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">7.</span> <span class="toc-text">在线升级</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">8.</span> <span class="toc-text">RAM空间占用</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">9.</span> <span class="toc-text">运行时间</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#undefined"><span class="toc-number">10.</span> <span class="toc-text">参考</span></a></li></ol></div></div><div class="author-info hide"><div class="author-info__avatar text-center"><img src="http://7xiuu0.com1.z0.glb.clouddn.com/17-11-23/22222257.jpg"></div><div class="author-info__name text-center">Wente Guo</div><div class="author-info__description text-center">一个学习的博客网站，专注于计算机科学、机器学习、数据挖掘等的 技术分享、相关咨询及经验总结</div><hr><div class="author-info-articles"><a class="author-info-articles__archives article-meta" href="/archives"><span class="pull-left">文章</span><span class="pull-right">134</span></a><a class="author-info-articles__tags article-meta" href="/tags"><span class="pull-left">标签</span><span class="pull-right">184</span></a><a class="author-info-articles__categories article-meta" href="/categories"><span class="pull-left">分类</span><span class="pull-right">19</span></a></div></div></div><div id="content-outer"><div class="no-bg" id="top-container" style="background-image: url(true);"><div id="page-header"><span class="pull-left"> <a id="site-name" href="/">CS Learning of CHNwentao</a></span><i class="fa fa-bars toggle-menu pull-right" aria-hidden="true"></i><span class="pull-right menus"><a class="site-page" href="/">Home</a><a class="site-page" href="/archives">Archives</a><a class="site-page" href="/tags">Tags</a><a class="site-page" href="/categories">Categories</a><a class="site-page" href="/about">About</a></span></div><div id="post-info"><div id="post-title">基于 Gensim 的 Word2Vec 实践</div><div id="post-meta"><time class="post-meta__date"><i class="fa fa-calendar" aria-hidden="true"></i> 2017-01-21</time><span class="post-meta__separator">|</span><i class="fa fa-inbox post-meta__icon" aria-hidden="true"> </i><a class="post-meta__categories" href="/categories/数据挖掘/">数据挖掘</a><div class="post-meta-wordcount"><span>字数总计: </span><span class="word-count">1,210</span><span class="post-meta__separator">|</span><span>阅读时长: 5 分钟</span></div></div></div></div><div class="layout" id="content-inner"><article id="post"><div id="post-content"><h2>模型原理</h2>
<!-- hexo-inject:begin --><!-- hexo-inject:end --><p>向量空间模型(Vector space model, VSM)将词语表示为一个连续的词向量, 且语义接近的单词对应的词向量在空间上(欧式距离, cosine相似度等)也是接近的.
&lt;!-- more --&gt;</p>
<p>VSM基于分布假说, 即若两个单词的上下文相同则两个单词的语义也相同.</p>
<p>基于分布假说的生成词向量的方法分为两大类:</p>
<p>统计法(count-based methods), 比如潜在语义分析(Latent Semantic Analysis, LSA)</p>
<p>预测法(predictive methods), 比如神经网络语言模型(Neural Network Language Model)</p>
<p>word2vec就是一种典型的神经网络语言模型.</p>
<h2>准备工作</h2>
<h3>安装</h3>
<p>需要注意的是：</p>
<p><code>pip install Cpython</code> :不安装不能开启并发</p>
<p><code>pin install Pattern</code></p>
<h2>加载语料库</h2>
<p>Gensim的word2vec的输入是句子的list</p>
<p>对于大量的输入语料集或者需要整合磁盘上多个文件夹下的数据，我们可以以迭代器的方式而不是一次性将全部内容读取到内存中来节省 RAM 空间：</p>
<p><figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">sentences_generator</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, filename)</span>:</span></span><br><span class="line">        self.dirname = dirname</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__iter__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">for</span> fname <span class="keyword">in</span> os.listdir(self.dirname):</span><br><span class="line">            <span class="keyword">for</span> line <span class="keyword">in</span> open(os.path.join(self.dirname, fname)):</span><br><span class="line">                sentence = line.strip().decode(<span class="string">'utf-8'</span>).split(<span class="string">' '</span>)</span><br><span class="line">                <span class="keyword">yield</span> sentence</span><br><span class="line"></span><br><span class="line">sent = sentences_generator(<span class="string">'/you/file/name'</span>)</span><br></pre></td></tr></table></figure></p>
<h2>参数的设定</h2>
<h3>min_count</h3>
<p><code>model = Word2Vec(sentences, min_count=10) # default value is 5</code></p>
<p>在不同大小的语料集中，我们对于基准词频的需求也是不一样的。譬如在较大的语料集中，我们希望忽略那些只出现过一两次的单词，这里我们就可以通过设置min_count参数进行控制。一般而言，合理的参数值会设置在0~100之间。</p>
<h3>size</h3>
<p>size参数主要是用来设置神经网络的层数，Word2Vec 中的默认值是设置为100层。更大的层次设置意味着更多的输入数据，不过也能提升整体的准确度，合理的设置范围为 10~数百。</p>
<p><code>model = Word2Vec(sentences, size=200) # default value is 100</code></p>
<h3>workers</h3>
<p>workers参数用于设置并发训练时候的线程数，不过仅当Cython安装的情况下才会起作用：</p>
<p><code>model = Word2Vec(sentences, workers=4) # default = 1 worker = no paralleliz</code></p>
<h2>存储和加载模型</h2>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>model.save(<span class="string">'/tmp/mymodel'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>new_model = gensim.models.Word2Vec.load(<span class="string">'/tmp/mymodel'</span>)</span><br></pre></td></tr></table></figure></p>
<h3>直接储存或加载由C生成的模型:</h3>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">model.save_word2vec_format(<span class="string">'./baike.zh.text.vector'</span>, binary=<span class="keyword">False</span>)</span><br><span class="line"></span><br><span class="line">model = Word2Vec.load_word2vec_format(<span class="string">'/tmp/vectors.txt'</span>, binary=<span class="keyword">False</span>)</span><br><span class="line"><span class="comment"># using gzipped/bz2 input works too, no need to unzip:</span></span><br><span class="line">model = Word2Vec.load_word2vec_format(<span class="string">'/tmp/vectors.bin.gz'</span>, binary=<span class="keyword">True</span>)</span><br></pre></td></tr></table></figure></p>
<h2>训练</h2>
<p>word2vec会在整个数据集上跑两遍：</p>
<ul>
<li>
<p>第一遍会收集单词及其词频来构建一个内部字典树结构.</p>
</li>
<li>
<p>第二遍训练神经网络.</p>
</li>
</ul>
<p>只能遍历一边数据, 则可以参考以下做法分步训练的方法 :</p>
<p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; model = gensim.models.Word2Vec() # an empty model, no training</span><br><span class="line">&gt;&gt;&gt; model.build_vocab(some_sentences)  # can be a non-repeatable, 1-pass generator</span><br><span class="line">&gt;&gt;&gt; model.train(other_sentences)  # can be a non-repeatable, 1-pass generator</span><br></pre></td></tr></table></figure></p>
<h2>在线升级</h2>
<p>可以在加载模型之后使用另外的句子来进一步训练模型</p>
<p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>model = gensim.models.Word2Vec.load(<span class="string">'/tmp/mymodel'</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>model.train(more_sentences)</span><br></pre></td></tr></table></figure></p>
<h2>RAM空间占用</h2>
<p>官方：</p>
<blockquote>
<p>Every 10 million word types need about 1GB of RAM.</p>
</blockquote>
<p>有的人的说法：</p>
<p>word2vec的参数被存储为矩阵(Numpy array). array的大小为<code>vocabulary</code>  乘以 <code>size</code> 大小的浮点数(4 byte)矩阵.</p>
<p>内存中有三个这样的矩阵, 如果你的输入包含100,000个单词, 隐层单元数为200, 则需要的内存大小为100,000 * 200 * 4 * 3 bytes, 约为229MB.</p>
<p>另外还需要一些内存来存储字典树, 但是除非你的单词是特别长的字符串, 大部分内存占用都来自前面说的三个矩阵.</p>
<p>The simplest solution is probably to increase min_count or decrease size</p>
<h2>运行时间</h2>
<p>word2vec会在整个句子序列上跑两遍, 第一遍会收集单词及其词频来够爱走一个内部字典树结构. 第二遍才会训练神经网络.</p>
<p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">2017-01-24 01:57:42: INFO: work start!</span><br><span class="line">2017-01-24 03:37:28: INFO: collected 25703385 word types from a corpus of 6380177283 raw words and 16408256 sentences</span><br><span class="line">2017-01-24 03:37:28: INFO: Loading a fresh vocabulary</span><br><span class="line">2017-01-24 03:41:15: INFO: min_count=10 retains 2501759 unique words (9% of original 25703385, drops 23201626)</span><br><span class="line">2017-01-24 03:41:15: INFO: min_count=10 leaves 6336057704 word corpus (99% of original 6380177283, drops 44119579)</span><br><span class="line">2017-01-24 03:41:29: INFO: deleting the raw counts dictionary of 25703385 items</span><br><span class="line">2017-01-24 03:41:44: INFO: sample=0.001 downsamples 29 most-common words</span><br><span class="line">2017-01-24 03:41:44: INFO: downsampling leaves estimated 5284735896 word corpus (83.4% of prior 6336057704)</span><br><span class="line">2017-01-24 03:41:44: INFO: estimated required memory for 2501759 words and 300 dimensions: 7255101100 bytes</span><br><span class="line">2017-01-24 03:42:07: INFO: resetting layer weights</span><br><span class="line">2017-01-24 03:43:06: INFO: training model with 4 workers on 2501759 vocabulary and 300 features, using sg=0 hs=0 sample=0.001 negative=5 window=5</span><br><span class="line">2017-01-24 03:43:06: INFO: expecting 16408256 sentences, matching count from corpus used for vocabulary survey</span><br><span class="line">​</span><br></pre></td></tr></table></figure></p>
<h2>参考</h2>
<ul>
<li><a href="http://www.cnblogs.com/Finley/p/6043769.html" target="_blank" rel="noopener">word2vec模型原理与实现</a></li>
</ul>
</div><div class="post-copyright"><div class="post-copyright__author"><span class="post-copyright-meta">文章作者: </span><span class="post-copyright-info">Wente Guo</span></div><div class="post-copyright__type"><span class="post-copyright-meta">文章链接: </span><span class="post-copyright-info">http://chnwentao.com/2017/01/待整理/NLP-word2vec学习/</span></div><div class="post-copyright__notice"><span class="post-copyright-meta">版权声明: </span><span class="post-copyright-info">本博客所有文章除特别声明外，均采用 <a href="https://creativecommons.org/licenses/by-nc-sa/3.0/" target="_blank">CC BY-NC-SA 3.0</a> 许可协议。转载请注明出处！</span></div></div><div class="post-meta__tag-list"><a class="post-meta__tags" href="/tags/Word2Vec/">Word2Vec</a><a class="post-meta__tags" href="/tags/Gensim/">Gensim</a></div></article><nav id="pagination"><div class="prev-post pull-left"><a href="/2017/02/数据挖掘/DM-特征工程-多值分类变量/"><i class="fa fa-chevron-left">  </i><span>数据预处理——多值分类变量</span></a></div><div class="next-post pull-right"><a href="/2016/12/待整理/NLP-做一个自己的聊天机器人/"><span>一步一步做一个自己的聊天机器人——— Word2Vec建模</span><i class="fa fa-chevron-right"></i></a></div></nav></div></div><footer><div class="layout" id="footer"><div class="copyright">©2015 - 2017 By Wente Guo</div><div class="framework-info"><span>驱动 - </span><a href="http://hexo.io"><span>Hexo</span></a><span class="footer-separator">|</span><span>主题 - </span><a href="https://github.com/Molunerfinn/hexo-theme-melody"><span>Melody</span></a></div><div class="busuanzi"><script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script><span id="busuanzi_container_page_pv"><i class="fa fa-file-o"></i><span id="busuanzi_value_page_pv"></span><span></span></span></div></div></footer><i class="fa fa-arrow-up" id="go-up" aria-hidden="true"></i><script src="/js/third-party/anime.min.js"></script><script src="/js/third-party/jquery.min.js"></script><script src="/js/third-party/jquery.fancybox.min.js"></script><script src="/js/third-party/velocity.min.js"></script><script src="/js/third-party/velocity.ui.min.js"></script><script src="/js/utils.js?version=1.3"></script><script src="/js/fancybox.js?version=1.3"></script><script src="/js/sidebar.js?version=1.3"></script><script src="/js/copy.js?version=1.3"></script><script src="/js/fireworks.js?version=1.3"></script><script src="/js/transition.js?version=1.3"></script><script src="/js/scroll.js?version=1.3"></script><script src="/js/head.js?version=1.3"></script><script type="text/x-mathjax-config">MathJax.Hub.Config({
  tex2jax: {
    inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
    processEscapes: true,
    skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
  }
});
</script><script type="text/x-mathjax-config">MathJax.Hub.Queue(function() {
  var all = MathJax.Hub.getAllJax(), i;
  for (i=0; i < all.length; i += 1) {
    all[i].SourceElement().parentNode.className += ' has-jax';
  }
});
</script><script src="https://cdn.bootcss.com/mathjax/2.7.2/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}}});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end --></body></html>